{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameliamansfield/EMR_Data_SepsisAnalysis_Python/blob/main/SepsisAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "3_vv5-qV-2C0",
        "outputId": "d092fcd6-af91-4ad3-bda4-f00437836e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "unable to open database file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-625516770.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eicu_rebuilt.sqlite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/2025_PatientLevelDataAnalysis/eicu_csvs\"\n",
        "\n",
        "db_path = os.path.join(folder_path, \"eicu_rebuilt.sqlite\")\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/2025_PatientLevelDataAnalysis/eicu_csvs/patient.csv.gz\")\n",
        "df.to_sql(\"patient\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "print(cursor.fetchall())\n",
        "\n",
        "for file in os.listdir(folder_path):\n",
        "    if file.endswith(\".csv.gz\"):\n",
        "        table_name = file.replace(\".csv.gz\", \"\")\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        print(f\"Loading {table_name}...\")\n",
        "\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        if not df.empty:\n",
        "            df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = [t[0] for t in cursor.fetchall()]\n",
        "print(\"Tables in database:\", tables)\n",
        "\n",
        "for table in tables:\n",
        "    cursor.execute(f\"PRAGMA table_info({table});\")\n",
        "    columns = [col[1] for col in cursor.fetchall()]\n",
        "    print(f\"\\nTable: {table}\\nColumns: {columns}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "all_files = os.listdir(\"/content/drive/MyDrive/2025_PatientLevelDataAnalysis/eicu_csvs/\")\n",
        "\n",
        "def pick(files, exact=None, include=None, exclude=None):\n",
        "    files_l = [f.lower() for f in files]\n",
        "    mapping = dict(zip(files_l, files))\n",
        "    if exact and exact.lower() in mapping:\n",
        "        return mapping[exact.lower()]\n",
        "    cand = []\n",
        "    for fl, orig in mapping.items():\n",
        "        if include and not any(inc in fl for inc in include):\n",
        "            continue\n",
        "        if exclude and any(exc in fl for exc in exclude):\n",
        "            continue\n",
        "        cand.append(orig)\n",
        "    return cand[0] if cand else None\n",
        "\n",
        "file_patient   = pick(all_files, exact='patient.csv.gz', include=['patient'], exclude=['apache'])\n",
        "file_diagnosis = pick(all_files, exact='diagnosis.csv.gz', include=['diagnosis','admissiondx'])\n",
        "file_apache_pt = pick(all_files, exact='apachePatientResult.csv.gz', include=['apachepatientresult'])\n",
        "file_vital_per = pick(all_files, exact='vitalPeriodic.csv.gz', include=['vitalperiodic'])\n",
        "file_vital_aper= pick(all_files, exact='vitalAperiodic.csv.gz', include=['vitalaperiodic'])\n",
        "file_lab       = pick(all_files, exact='lab.csv.gz', include=['lab'], exclude=['microlab','customlab'])\n",
        "file_past_hist = pick(all_files, exact='pastHistory.csv.gz', include=['pasthistory'])\n",
        "\n",
        "print(\"Tables:\")\n",
        "print(\"  patient        :\", file_patient)\n",
        "print(\"  diagnosis      :\", file_diagnosis)\n",
        "print(\"  apache results :\", file_apache_pt)\n",
        "print(\"  vitalPeriodic  :\", file_vital_per)\n",
        "print(\"  vitalAperiodic :\", file_vital_aper)\n",
        "print(\"  lab            :\", file_lab)\n",
        "print(\"  pastHistory    :\", file_past_hist)\n",
        "\n",
        "assert file_patient and file_diagnosis, \"Missing required tables (patient/diagnosis).\""
      ],
      "metadata": {
        "id": "Vlhu2g86Aaur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_path = \"/content/drive/MyDrive/2025_PatientLevelDataAnalysis/eicu_csvs/eicu_rebuilt.sqlite\"\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "def load_to_sql(csv_file, table_name, conn, chunksize=100000):\n",
        "    full_path = os.path.join(\"/content/drive/MyDrive/2025_PatientLevelDataAnalysis/eicu_csvs/\", csv_file)\n",
        "    for chunk in pd.read_csv(full_path, compression=\"gzip\", chunksize=chunksize, low_memory=False):\n",
        "        chunk.to_sql(table_name, conn, if_exists=\"append\", index=False)\n",
        "\n",
        "def ensure_table(csv_file, table_name):\n",
        "    if csv_file is None:\n",
        "        return\n",
        "    check = conn.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'\").fetchone()\n",
        "    if not check:\n",
        "        print(f\"Loading {table_name} ...\")\n",
        "        load_to_sql(csv_file, table_name, conn)\n",
        "    else:\n",
        "        print(f\"Table {table_name} already exists\")\n",
        "\n",
        "ensure_table(file_patient, \"patient\")\n",
        "ensure_table(file_diagnosis, \"diagnosis\")\n",
        "ensure_table(file_apache_pt, \"apachePatientResult\")\n",
        "ensure_table(file_vital_per, \"vitalPeriodic\")\n",
        "ensure_table(file_vital_aper, \"vitalAperiodic\")\n",
        "ensure_table(file_lab, \"lab\")\n",
        "ensure_table(file_past_hist, \"pastHistory\")"
      ],
      "metadata": {
        "id": "mGKe8xbbAfjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patient\n",
        "patient = pd.read_sql_query(\"SELECT * FROM patient\", conn)\n",
        "\n",
        "# Apache\n",
        "apache = pd.read_sql_query(\"SELECT * FROM apachePatientResult\", conn)\n",
        "\n",
        "# Vital periodic\n",
        "vitals_periodic = pd.read_sql_query(\"SELECT * FROM vitalPeriodic\", conn)\n",
        "vitals_periodic_agg = vitals_periodic.groupby(\"patientunitstayid\").agg({\n",
        "    \"heartrate\": [\"mean\", \"max\", \"min\"],\n",
        "    \"temperature\": [\"mean\", \"max\", \"min\"],\n",
        "    \"respiration\": [\"mean\", \"max\", \"min\"],\n",
        "    \"sao2\": [\"mean\", \"min\"]\n",
        "}).reset_index()\n",
        "vitals_periodic_agg.columns = [\"_\".join(col).strip(\"_\") for col in vitals_periodic_agg.columns]\n",
        "\n",
        "# Vital aperiodic\n",
        "vitals_aperiodic = pd.read_sql_query(\"SELECT * FROM vitalAperiodic\", conn)\n",
        "vitals_aperiodic_agg = vitals_aperiodic.groupby(\"patientunitstayid\").agg({\n",
        "    \"noninvasivesystolic\": [\"mean\", \"max\"],\n",
        "    \"noninvasivediastolic\": [\"mean\", \"min\"],\n",
        "    \"noninvasivemean\": [\"mean\"],\n",
        "    \"paop\": [\"mean\"]\n",
        "}).reset_index()\n",
        "vitals_aperiodic_agg.columns = [\"_\".join(col).strip(\"_\") for col in vitals_aperiodic_agg.columns]\n",
        "\n",
        "# Labs\n",
        "lab = pd.read_sql_query(\"SELECT patientunitstayid, labname, labresult FROM lab\", conn)\n",
        "lab_agg = lab.groupby([\"patientunitstayid\", \"labname\"]).agg({\"labresult\": \"mean\"}).unstack()\n",
        "lab_agg.columns = [f\"{lab}_{agg}\" for lab, agg in lab_agg.columns]\n",
        "lab_agg = lab_agg.reset_index()\n",
        "\n",
        "# Final merged dataset\n",
        "df = (patient\n",
        "      .merge(apache, on=\"patientunitstayid\", how=\"left\")\n",
        "      .merge(vitals_periodic_agg, on=\"patientunitstayid\", how=\"left\")\n",
        "      .merge(vitals_aperiodic_agg, on=\"patientunitstayid\", how=\"left\")\n",
        "      .merge(lab_agg, on=\"patientunitstayid\", how=\"left\"))\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LrvPt3AdAtqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/2025_PatientLevelDataAnalysis/eicu_csvs\"\n",
        "\n",
        "def pick(files, exact=None, include=None, exclude=None):\n",
        "    files_l = [f.lower() for f in files]\n",
        "    mapping = dict(zip(files_l, files))\n",
        "    if exact and exact.lower() in mapping:\n",
        "        return mapping[exact.lower()]\n",
        "    cand = []\n",
        "    for fl, orig in mapping.items():\n",
        "        if include and not any(inc in fl for inc in include):\n",
        "            continue\n",
        "        if exclude and any(exc in fl for exc in exclude):\n",
        "            continue\n",
        "        cand.append(orig)\n",
        "    return cand[0] if cand else None\n",
        "\n",
        "def read_csv_any(path, nrows=2000):\n",
        "    if path is None: return None\n",
        "    return pd.read_csv(path, nrows=nrows)\n",
        "\n",
        "def clean(df):\n",
        "    if df is None: return None\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "    return df\n",
        "\n",
        "KEY_CAND = ['patientunitstayid','admissionid','stay_id','hadm_id','encounterid']\n",
        "def find_key(cols):\n",
        "    for k in KEY_CAND:\n",
        "        if k in cols: return k\n",
        "    return None\n",
        "\n",
        "def find_offset_col(df):\n",
        "    if df is None: return None\n",
        "    cands = [c for c in df.columns if 'offset' in c]\n",
        "    pref = ['observationoffset','labresultoffset','unitadmitoffset','icuadmitoffset','hospitaladmitoffset']\n",
        "    for p in pref:\n",
        "        if p in cands: return p\n",
        "    return cands[0] if cands else None\n",
        "\n",
        "pf  = os.path.join(BASE_DIR, file_patient)\n",
        "df  = os.path.join(BASE_DIR, file_diagnosis)\n",
        "apf = os.path.join(BASE_DIR, file_apache_pt) if file_apache_pt else None\n",
        "vpf = os.path.join(BASE_DIR, file_vital_per) if file_vital_per else None\n",
        "vaf = os.path.join(BASE_DIR, file_vital_aper) if file_vital_aper else None\n",
        "lf  = os.path.join(BASE_DIR, file_lab) if file_lab else None\n",
        "phf = os.path.join(BASE_DIR, file_past_hist) if file_past_hist else None\n",
        "\n",
        "patients_s   = clean(read_csv_any(pf))\n",
        "diagnoses_s  = clean(read_csv_any(df))\n",
        "apache_s     = clean(read_csv_any(apf))\n",
        "vitalp_s     = clean(read_csv_any(vpf))\n",
        "vitala_s     = clean(read_csv_any(vaf))\n",
        "lab_s        = clean(read_csv_any(lf))\n",
        "pasthist_s   = clean(read_csv_any(phf))\n",
        "\n",
        "KEY = find_key(set(patients_s.columns))\n",
        "assert KEY and KEY in diagnoses_s.columns, \"Join key not found in patient/diagnosis.\"\n",
        "\n",
        "PATIENT_ADMIT_OFFSET = find_offset_col(patients_s)\n",
        "print(\"Detected join key:\", KEY)\n",
        "print(\"Patient admit offset column:\", PATIENT_ADMIT_OFFSET)\n",
        "\n",
        "db_path = os.path.join(BASE_DIR, \"eicu_rebuilt.sqlite\")\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "patient = pd.read_sql_query(\"SELECT * FROM patient\", conn)\n",
        "apache  = pd.read_sql_query(\"SELECT * FROM apachePatientResult\", conn)\n",
        "vitalp  = pd.read_sql_query(\"SELECT * FROM vitalPeriodic\", conn)\n",
        "vitala  = pd.read_sql_query(\"SELECT * FROM vitalAperiodic\", conn)\n",
        "lab     = pd.read_sql_query(\"SELECT patientunitstayid, labname, labresult FROM lab\", conn)\n",
        "\n",
        "vitalp_agg = vitalp.groupby(\"patientunitstayid\").agg({\n",
        "    \"heartrate\": [\"mean\", \"max\", \"min\"],\n",
        "    \"temperature\": [\"mean\", \"max\", \"min\"],\n",
        "    \"respiration\": [\"mean\", \"max\", \"min\"],\n",
        "    \"sao2\": [\"mean\", \"min\"]\n",
        "}).reset_index()\n",
        "vitalp_agg.columns = [\"_\".join(col).strip(\"_\") for col in vitalp_agg.columns]\n",
        "\n",
        "vitala_agg = vitala.groupby(\"patientunitstayid\").agg({\n",
        "    \"noninvasivesystolic\": [\"mean\", \"max\"],\n",
        "    \"noninvasivediastolic\": [\"mean\", \"min\"],\n",
        "    \"noninvasivemean\": [\"mean\"],\n",
        "    \"paop\": [\"mean\"]\n",
        "}).reset_index()\n",
        "vitala_agg.columns = [\"_\".join(col).strip(\"_\") for col in vitala_agg.columns]\n",
        "\n",
        "lab_agg = lab.groupby([\"patientunitstayid\", \"labname\"]).agg({\"labresult\": \"mean\"}).unstack()\n",
        "lab_agg.columns = [f\"{lab}_{agg}\" for lab, agg in lab_agg.columns]\n",
        "lab_agg = lab_agg.reset_index()\n",
        "\n",
        "df = (patient\n",
        "      .merge(apache, on=\"patientunitstayid\", how=\"left\")\n",
        "      .merge(vitalp_agg, on=\"patientunitstayid\", how=\"left\")\n",
        "      .merge(vitala_agg, on=\"patientunitstayid\", how=\"left\")\n",
        "      .merge(lab_agg, on=\"patientunitstayid\", how=\"left\"))\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QNUKy933BbUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare(pf, df, apf=None, vpf=None, vaf=None, lf=None, phf=None, sample_n=2000):\n",
        "    \"\"\"Load eICU tables: first a sample for inspection, then the full dataset.\"\"\"\n",
        "\n",
        "    def clean(df):\n",
        "        df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "        return df\n",
        "\n",
        "    patients_s   = read_csv_any(pf, nrows=sample_n)\n",
        "    diagnoses_s  = read_csv_any(df, nrows=sample_n)\n",
        "    apache_s     = read_csv_any(apf, nrows=sample_n) if apf else None\n",
        "    vitalp_s     = read_csv_any(vpf, nrows=sample_n) if vpf else None\n",
        "    vitala_s     = read_csv_any(vaf, nrows=sample_n) if vaf else None\n",
        "    lab_s        = read_csv_any(lf,  nrows=sample_n) if lf else None\n",
        "    pasthist_s   = read_csv_any(phf, nrows=sample_n) if phf else None\n",
        "\n",
        "    patients_s, diagnoses_s = clean(patients_s), clean(diagnoses_s)\n",
        "    if apache_s is not None: apache_s = clean(apache_s)\n",
        "    if vitalp_s is not None: vitalp_s = clean(vitalp_s)\n",
        "    if vitala_s is not None: vitala_s = clean(vitala_s)\n",
        "    if lab_s is not None: lab_s = clean(lab_s)\n",
        "    if pasthist_s is not None: pasthist_s = clean(pasthist_s)\n",
        "\n",
        "    KEY_CAND = ['patientunitstayid','admissionid','stay_id','hadm_id','encounterid']\n",
        "    def find_key(cols):\n",
        "        for k in KEY_CAND:\n",
        "            if k in cols: return k\n",
        "        return None\n",
        "\n",
        "    KEY = find_key(set(patients_s.columns))\n",
        "    assert KEY and KEY in diagnoses_s.columns, \"Join key not found in patient/diagnosis.\"\n",
        "\n",
        "    def find_offset_col(df):\n",
        "        if df is None: return None\n",
        "        cands = [c for c in df.columns if 'offset' in c]\n",
        "        pref = ['observationoffset','labresultoffset','unitadmitoffset','icuadmitoffset','hospitaladmitoffset']\n",
        "        for p in pref:\n",
        "            if p in cands: return p\n",
        "        return cands[0] if cands else None\n",
        "\n",
        "    PATIENT_ADMIT_OFFSET = find_offset_col(patients_s)\n",
        "    print(\"Detected key:\", KEY, \"| patient admit offset column:\", PATIENT_ADMIT_OFFSET)\n",
        "\n",
        "    patients   = clean(read_csv_any(pf))\n",
        "    diagnoses  = clean(read_csv_any(df))\n",
        "    apache_pt  = clean(read_csv_any(apf)) if apf else None\n",
        "    vital_per  = clean(read_csv_any(vpf)) if vpf else None\n",
        "    vital_aper = clean(read_csv_any(vaf)) if vaf else None\n",
        "    lab        = clean(read_csv_any(lf))  if lf else None\n",
        "    past_hist  = clean(read_csv_any(phf)) if phf else None\n",
        "\n",
        "    print(\"Shapes -> patients:\", patients.shape, \"diagnoses:\", diagnoses.shape)\n",
        "\n",
        "    return {\n",
        "        \"patients\": patients,\n",
        "        \"diagnoses\": diagnoses,\n",
        "        \"apache_pt\": apache_pt,\n",
        "        \"vital_per\": vital_per,\n",
        "        \"vital_aper\": vital_aper,\n",
        "        \"lab\": lab,\n",
        "        \"past_hist\": past_hist,\n",
        "        \"key\": KEY,\n",
        "        \"patient_admit_offset\": PATIENT_ADMIT_OFFSET\n",
        "    }"
      ],
      "metadata": {
        "id": "wXFr_wrBB07j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# uses an existing SQLite connection named `conn`\n",
        "\n",
        "def read_csv_any(path_or_df, nrows=None):\n",
        "    \"\"\"Accepts a file path (csv/tsv/csv.gz) OR a pandas DataFrame.\"\"\"\n",
        "    if path_or_df is None:\n",
        "        return None\n",
        "    if isinstance(path_or_df, pd.DataFrame):\n",
        "        return path_or_df if nrows is None else path_or_df.head(nrows)\n",
        "    path = path_or_df\n",
        "    compression = 'gzip' if str(path).endswith('.gz') else 'infer'\n",
        "    sep = '\\t' if str(path).endswith('.tsv') else ','\n",
        "    return pd.read_csv(path, sep=sep, compression=compression, nrows=nrows)\n",
        "\n",
        "def clean(df):\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def load_to_sql(df, name):\n",
        "    if df is not None:\n",
        "        df.to_sql(name, conn, if_exists=\"replace\", index=False)\n",
        "        print(f\"Loaded table: {name} ({df.shape[0]} rows)\")\n",
        "    return df\n",
        "\n",
        "def load_and_prepare(pf, diagf, apf=None, vpf=None, vaf=None, lf=None, phf=None, nrows=None):\n",
        "    \"\"\"pf/diagf/... should be FILE PATHS (or DataFrames).\"\"\"\n",
        "    patients   = load_to_sql(clean(read_csv_any(pf,   nrows=nrows)), \"patients\")\n",
        "    diagnoses  = load_to_sql(clean(read_csv_any(diagf,nrows=nrows)), \"diagnoses\")\n",
        "    apache_pt  = load_to_sql(clean(read_csv_any(apf,  nrows=nrows)), \"apache_pt\")   if apf else None\n",
        "    vital_per  = load_to_sql(clean(read_csv_any(vpf,  nrows=nrows)), \"vital_per\")   if vpf else None\n",
        "    vital_aper = load_to_sql(clean(read_csv_any(vaf,  nrows=nrows)), \"vital_aper\")  if vaf else None\n",
        "    lab        = load_to_sql(clean(read_csv_any(lf,   nrows=nrows)), \"lab\")         if lf else None\n",
        "    past_hist  = load_to_sql(clean(read_csv_any(phf,  nrows=nrows)), \"past_hist\")   if phf else None\n",
        "    return {\n",
        "        \"patients\": patients, \"diagnoses\": diagnoses, \"apache_pt\": apache_pt,\n",
        "        \"vital_per\": vital_per, \"vital_aper\": vital_aper, \"lab\": lab,\n",
        "        \"past_hist\": past_hist, \"key\": \"patientunitstayid\"\n",
        "    }\n",
        "\n",
        "    pf     = os.path.join(BASE_DIR, file_patient)\n",
        "diagf  = os.path.join(BASE_DIR, file_diagnosis)   # <-- use diagf, NOT a DataFrame\n",
        "apf    = os.path.join(BASE_DIR, file_apache_pt) if file_apache_pt else None\n",
        "vpf    = os.path.join(BASE_DIR, file_vital_per)  if file_vital_per else None\n",
        "vaf    = os.path.join(BASE_DIR, file_vital_aper) if file_vital_aper else None\n",
        "lf     = os.path.join(BASE_DIR, file_lab)        if file_lab else None\n",
        "phf    = os.path.join(BASE_DIR, file_past_hist)  if file_past_hist else None\n",
        "\n",
        "tables = load_and_prepare(pf, diagf, apf, vpf, vaf, lf, phf)\n",
        "\n",
        "pd.read_sql(\"SELECT patientunitstayid, gender, age FROM patients LIMIT 5\", conn)"
      ],
      "metadata": {
        "id": "ThWQI-VqH5F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3, pandas as pd\n",
        "\n",
        "conn = sqlite3.connect(\"your_database.db\")\n",
        "\n",
        "patients = pd.read_sql(\"SELECT * FROM patients\", conn)\n",
        "diagnoses = pd.read_sql(\"SELECT * FROM diagnoses\", conn)\n",
        "\n",
        "print(\"Patients shape:\", patients.shape)\n",
        "print(\"Diagnoses shape:\", diagnoses.shape)\n",
        "print(diagnoses.head())\n",
        "\n",
        "diagnoses_expanded = (\n",
        "    diagnoses.assign(icd9code=diagnoses['icd9code'].str.split(','))\n",
        "             .explode('icd9code')\n",
        "             .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "diagnoses_expanded['icd9code'] = (\n",
        "    diagnoses_expanded['icd9code'].str.strip()\n",
        ")\n",
        "\n",
        "print(\"Original shape:\", diagnoses.shape)\n",
        "print(\"Expanded shape:\", diagnoses_expanded.shape)\n",
        "print(diagnoses_expanded.head(10))"
      ],
      "metadata": {
        "id": "NtFlD17-KV5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "OUT = os.path.join(BASE_DIR, 'outputs')\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "KEY = \"patientunitstayid\"\n",
        "\n",
        "diagnoses_expanded = (\n",
        "    diagnoses.assign(icd_code=diagnoses['icd9code'].str.split(','))\n",
        "             .explode('icd_code')\n",
        "             .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "diagnoses_expanded['icd_code'] = (\n",
        "    diagnoses_expanded['icd_code']\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.replace('.', '', regex=False)\n",
        "    .str.upper()\n",
        ")\n",
        "\n",
        "print(\"Original shape:\", diagnoses.shape)\n",
        "print(\"Expanded shape:\", diagnoses_expanded.shape)\n",
        "print(diagnoses_expanded.head(10))\n",
        "\n",
        "dx_counts = (\n",
        "    diagnoses_expanded.groupby(\"icd_code\")[KEY].nunique()\n",
        "    .sort_values(ascending=False)\n",
        "    .reset_index()\n",
        "    .rename(columns={KEY: \"unique_stays\"})\n",
        ")\n",
        "dx_counts.to_csv(os.path.join(OUT, \"diagnosis_frequency.csv\"), index=False)\n",
        "\n",
        "CURATED_GROUPS = {\n",
        "    'sepsis':        ['A41','038','R652','9959'],\n",
        "    'pneumonia':     ['J18','486'],\n",
        "    'ami':           ['I21','410'],\n",
        "    'aki':           ['N17','584'],\n",
        "    'chf':           ['I50','428'],\n",
        "    'copd':          ['J44','496'],\n",
        "    'stroke':        ['I63','I61','434','431'],\n",
        "    'resp_failure':  ['J96','51881'],\n",
        "    'gi_bleed':      ['K922','578'],\n",
        "    'dka':           ['E101','E111','2501'],\n",
        "    'shock':         ['R57','7855'],\n",
        "    'uti':           ['N390','5990'],\n",
        "    'ards':          ['J80','51882'],\n",
        "}\n",
        "\n",
        "def code_to_curated_group(code):\n",
        "    for grp, prefs in CURATED_GROUPS.items():\n",
        "        if any(code.startswith(p) for p in prefs):\n",
        "            return grp\n",
        "    return None\n",
        "\n",
        "diag_grp = diagnoses_expanded[[KEY, \"icd_code\"]].copy()\n",
        "diag_grp[\"curated_group\"] = diag_grp[\"icd_code\"].apply(code_to_curated_group)\n",
        "\n",
        "diag_grp[\"prefix3\"] = diag_grp[\"icd_code\"].str[:3]\n",
        "\n",
        "prefix_freq = (\n",
        "    diag_grp.groupby(\"prefix3\")[KEY].nunique()\n",
        "    .sort_values(ascending=False)\n",
        "    .reset_index()\n",
        "    .rename(columns={KEY: \"unique_stays\"})\n",
        ")\n",
        "prefix_freq.to_csv(os.path.join(OUT, \"diagnosis_prefix3_frequency.csv\"), index=False)\n",
        "\n",
        "TOP_PREFIX_K = 15\n",
        "top_prefixes = set(prefix_freq.head(TOP_PREFIX_K)[\"prefix3\"].tolist())\n",
        "\n",
        "labels = pd.DataFrame({KEY: patients[KEY].unique()})\n",
        "labels = labels.sort_values(KEY).reset_index(drop=True)\n",
        "\n",
        "for grp in CURATED_GROUPS.keys():\n",
        "    stays = diag_grp.loc[diag_grp[\"curated_group\"] == grp, KEY].unique()\n",
        "    labels[grp] = labels[KEY].isin(stays).astype(int)\n",
        "\n",
        "for p in sorted(top_prefixes):\n",
        "    stays = diag_grp.loc[diag_grp[\"prefix3\"] == p, KEY].unique()\n",
        "    labels[f\"icd_{p}\"] = labels[KEY].isin(stays).astype(int)\n",
        "\n",
        "MIN_POS = 25\n",
        "label_cols_all = [c for c in labels.columns if c != KEY]\n",
        "pos_counts = labels[label_cols_all].sum().sort_values(ascending=False)\n",
        "\n",
        "keep_cols = [c for c in label_cols_all if labels[c].sum() >= MIN_POS]\n",
        "dropped = sorted(set(label_cols_all) - set(keep_cols))\n",
        "\n",
        "labels = labels[[KEY] + keep_cols]\n",
        "\n",
        "print(\"Label columns kept:\", keep_cols)\n",
        "print(\"Dropped (too few positives):\", dropped)\n",
        "print(\"Positives by label:\")\n",
        "print(labels[keep_cols].sum().sort_values(ascending=False))\n",
        "\n",
        "labels.to_csv(os.path.join(OUT, \"labels_multi.csv\"), index=False)"
      ],
      "metadata": {
        "id": "J5h_Qc7GK4Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/2025_PatientLevelDataAnalysis/eicu_csvs\"\n",
        "db_path = os.path.join(BASE_DIR, \"eicu_rebuilt.sqlite\")\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "def pick(files, exact=None, include=None, exclude=None):\n",
        "    files_l = [f.lower() for f in files]\n",
        "    mapping = dict(zip(files_l, files))\n",
        "    if exact and exact.lower() in mapping:\n",
        "        return mapping[exact.lower()]\n",
        "    cand = []\n",
        "    for fl, orig in mapping.items():\n",
        "        if include and not any(inc in fl for inc in include):\n",
        "            continue\n",
        "        if exclude and any(exc in fl for exc in exclude):\n",
        "            continue\n",
        "        cand.append(orig)\n",
        "    return cand[0] if cand else None\n",
        "\n",
        "all_files = os.listdir(BASE_DIR)\n",
        "\n",
        "file_patient   = pick(all_files, exact='patient.csv.gz', include=['patient'], exclude=['apache'])\n",
        "file_diagnosis = pick(all_files, exact='diagnosis.csv.gz', include=['diagnosis','admissiondx'])\n",
        "file_apache_pt = pick(all_files, exact='apachePatientResult.csv.gz', include=['apachepatientresult'])\n",
        "file_vital_per = pick(all_files, exact='vitalPeriodic.csv.gz', include=['vitalperiodic'])\n",
        "file_vital_aper= pick(all_files, exact='vitalAperiodic.csv.gz', include=['vitalaperiodic'])\n",
        "file_lab       = pick(all_files, exact='lab.csv.gz', include=['lab'], exclude=['microlab','customlab'])\n",
        "file_past_hist = pick(all_files, exact='pastHistory.csv.gz', include=['pasthistory'])\n",
        "\n",
        "files_to_load = {\n",
        "    \"patients\": file_patient,\n",
        "    \"diagnoses\": file_diagnosis,\n",
        "    \"apache_pt\": file_apache_pt,\n",
        "    \"vital_per\": file_vital_per,\n",
        "    \"vital_aper\": file_vital_aper,\n",
        "    \"lab\": file_lab,\n",
        "    \"past_hist\": file_past_hist\n",
        "}\n",
        "\n",
        "for table_name, fname in files_to_load.items():\n",
        "    if fname is None:\n",
        "        print(f\"Skipping {table_name}: file not found\")\n",
        "        continue\n",
        "    path = os.path.join(BASE_DIR, fname)\n",
        "    print(f\"Loading {table_name} from {fname}...\")\n",
        "    df = pd.read_csv(path, compression='gzip' if fname.endswith('.gz') else None)\n",
        "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]  # normalize columns\n",
        "    df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
        "    print(f\"Loaded table: {table_name} ({df.shape[0]} rows, {df.shape[1]} cols)\")\n",
        "\n",
        "conn.close()\n",
        "print(\"All tables loaded into SQLite!\")\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/2025_PatientLevelDataAnalysis/eicu_csvs\"\n",
        "db_path = os.path.join(BASE_DIR, \"eicu_rebuilt.sqlite\")\n",
        "OUT = os.path.join(BASE_DIR, \"outputs\")\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "patients = pd.read_sql_query(\"SELECT * FROM patients\", conn)\n",
        "diagnoses = pd.read_sql_query(\"SELECT * FROM diagnoses\", conn)\n",
        "apache_pt = pd.read_sql_query(\"SELECT * FROM apache_pt\", conn)\n",
        "vital_per = pd.read_sql_query(\"SELECT * FROM vital_per\", conn)\n",
        "vital_aper = pd.read_sql_query(\"SELECT * FROM vital_aper\", conn)\n",
        "lab = pd.read_sql_query(\"SELECT * FROM lab\", conn)\n",
        "past_hist = pd.read_sql_query(\"SELECT * FROM past_hist\", conn)\n",
        "\n",
        "conn.close()\n",
        "\n",
        "KEY = \"patientunitstayid\"\n",
        "\n",
        "feat = patients[[KEY]].copy()\n",
        "for col in ['age','gender','ethnicity','unittype','unitstaytype','hospitaladmitsource','hospitaldischargestatus','unitdischargestatus']:\n",
        "    if col in patients.columns:\n",
        "        if col == 'age':\n",
        "            feat['age'] = pd.to_numeric(patients['age'], errors='coerce')\n",
        "        else:\n",
        "            feat[col] = patients[col].astype(str)\n",
        "\n",
        "admit_off, disc_off = None, None\n",
        "for c in ['unitadmitoffset','icuadmitoffset','hospitaladmitoffset']:\n",
        "    if c in patients.columns: admit_off = c; break\n",
        "for c in ['unitdischargeoffset','icudischargeoffset','hospitaldischargeoffset']:\n",
        "    if c in patients.columns: disc_off = c; break\n",
        "\n",
        "if admit_off and disc_off:\n",
        "    los_days = (pd.to_numeric(patients[disc_off], errors='coerce') -\n",
        "                pd.to_numeric(patients[admit_off], errors='coerce'))/(60*24)\n",
        "    feat['los_days'] = los_days\n",
        "\n",
        "if apache_pt is not None:\n",
        "    cols_ap = [c for c in apache_pt.columns if any(x in c for x in ['apache','predict','score','mortality'])]\n",
        "    if cols_ap:\n",
        "        ap = apache_pt[[KEY]+cols_ap].copy()\n",
        "        for c in cols_ap:\n",
        "            ap[c] = pd.to_numeric(ap[c], errors='ignore')\n",
        "        feat = feat.merge(ap, on=KEY, how='left')\n",
        "\n",
        "def aggregate_first_6h(df, key_col, offset_candidates, prefix):\n",
        "    if df is None: return None\n",
        "    off = None\n",
        "    for c in offset_candidates:\n",
        "        if c in df.columns: off = c; break\n",
        "    if off is None: return None\n",
        "    sub = df[(pd.to_numeric(df[off], errors='coerce') >= 0) & (pd.to_numeric(df[off], errors='coerce') <= 360)].copy()\n",
        "    if sub.empty: return None\n",
        "\n",
        "    num_cols = [c for c in sub.columns if c not in [key_col, off] and pd.api.types.is_numeric_dtype(sub[c])]\n",
        "    if not num_cols: return None\n",
        "    ag = sub.groupby(key_col)[num_cols].agg(['min','max','mean']).reset_index()\n",
        "    ag.columns = [key_col] + [f\"{prefix}__{c}__{stat}\" for c in num_cols for stat in ['min','max','mean']]\n",
        "    return ag\n",
        "\n",
        "vp_agg = aggregate_first_6h(vital_per, KEY, ['observationoffset','chartoffset','offset'], 'vitalp')\n",
        "if vp_agg is not None:\n",
        "    feat = feat.merge(vp_agg, on=KEY, how='left')\n",
        "\n",
        "va_agg = aggregate_first_6h(vital_aper, KEY, ['observationoffset','chartoffset','offset'], 'vitala')\n",
        "if va_agg is not None:\n",
        "    feat = feat.merge(va_agg, on=KEY, how='left')\n",
        "\n",
        "if lab is not None:\n",
        "    lb = lab.copy()\n",
        "    lb_off = next((c for c in ['labresultoffset','chartoffset','observationoffset','offset'] if c in lb.columns), None)\n",
        "    val_col = next((c for c in ['labresult','labvalue','resultvalue','value'] if c in lb.columns), None)\n",
        "    lname = 'labname' if 'labname' in lb.columns else None\n",
        "    if lb_off and val_col and lname:\n",
        "        lb[val_col] = pd.to_numeric(lb[val_col], errors='coerce')\n",
        "        common = (lb[lb[val_col].notna()].groupby(lname)[KEY].nunique().sort_values(ascending=False).head(20).index.tolist())\n",
        "        from functools import reduce\n",
        "        aggs = []\n",
        "        for ln in common:\n",
        "            sub = lb[(lb[lname]==ln) & lb[val_col].notna()]\n",
        "            ag = aggregate_first_6h(sub, KEY, [lb_off], f\"lab__{str(ln).replace(' ','_').lower()}\")\n",
        "            if ag is not None: aggs.append(ag)\n",
        "        if aggs:\n",
        "            feat = reduce(lambda L,R: L.merge(R,on=KEY,how='left'), [feat]+aggs)\n",
        "\n",
        "if past_hist is not None:\n",
        "    ph = past_hist.copy()\n",
        "    text_col = next((c for c in ['pasthistorytext','pasthistoryitem','pasthistoryvalue','pasthistorypath'] if c in ph.columns), None)\n",
        "    if text_col:\n",
        "        ph[text_col] = ph[text_col].astype(str).str.lower()\n",
        "        agg_txt = ph.groupby(KEY)[text_col].apply(lambda s: ' | '.join(s.astype(str))).reset_index()\n",
        "        flags = {\n",
        "            'hx_chf': ['heart failure','congestive heart','chf'],\n",
        "            'hx_copd': ['copd','chronic obstructive'],\n",
        "            'hx_ckd': ['ckd','chronic kidney','renal failure'],\n",
        "            'hx_dm':  ['diabetes','dm '],\n",
        "            'hx_cancer': ['cancer','malignancy','carcinoma','tumor'],\n",
        "        }\n",
        "        for col, kws in flags.items():\n",
        "            agg_txt[col] = agg_txt[text_col].apply(lambda t: any(k in t for k in kws))\n",
        "        feat = feat.merge(agg_txt.drop(columns=[text_col]), on=KEY, how='left')\n",
        "\n",
        "feat.to_csv(os.path.join(OUT, 'features_6h.csv'), index=False)\n",
        "print(\"6-hour feature matrix shape:\", feat.shape)"
      ],
      "metadata": {
        "id": "BC8EGu3TMosr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from functools import reduce\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/2025_PatientLevelDataAnalysis/eicu_csvs\"\n",
        "db_path = os.path.join(BASE_DIR, \"eicu_rebuilt.sqlite\")\n",
        "OUT = os.path.join(BASE_DIR, \"outputs\")\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "patients   = pd.read_sql_query(\"SELECT * FROM patients\", conn)\n",
        "diagnoses  = pd.read_sql_query(\"SELECT * FROM diagnoses\", conn)\n",
        "apache_pt  = pd.read_sql_query(\"SELECT * FROM apache_pt\", conn)\n",
        "vital_per  = pd.read_sql_query(\"SELECT * FROM vital_per\", conn)\n",
        "vital_aper = pd.read_sql_query(\"SELECT * FROM vital_aper\", conn)\n",
        "lab        = pd.read_sql_query(\"SELECT * FROM lab\", conn)\n",
        "past_hist  = pd.read_sql_query(\"SELECT * FROM past_hist\", conn)\n",
        "\n",
        "conn.close()\n",
        "\n",
        "KEY = \"patientunitstayid\"\n",
        "\n",
        "feat = patients[[KEY]].copy()\n",
        "\n",
        "for col in ['age','gender','ethnicity','unittype','unitstaytype','hospitaladmitsource',\n",
        "            'hospitaldischargestatus','unitdischargestatus']:\n",
        "    if col in patients.columns:\n",
        "        if col == 'age':\n",
        "            feat['age'] = pd.to_numeric(patients['age'], errors='coerce')\n",
        "        else:\n",
        "            feat[col] = patients[col].astype(str)\n",
        "\n",
        "admit_off = next((c for c in ['unitadmitoffset','icuadmitoffset','hospitaladmitoffset'] if c in patients.columns), None)\n",
        "disc_off  = next((c for c in ['unitdischargeoffset','icudischargeoffset','hospitaldischargeoffset'] if c in patients.columns), None)\n",
        "if admit_off and disc_off:\n",
        "    feat['los_days'] = (pd.to_numeric(patients[disc_off], errors='coerce') -\n",
        "                        pd.to_numeric(patients[admit_off], errors='coerce')) / (60*24)\n",
        "\n",
        "if apache_pt is not None:\n",
        "    cols_ap = [c for c in apache_pt.columns if any(x in c for x in ['apache','predict','score','mortality'])]\n",
        "    if cols_ap:\n",
        "        ap = apache_pt[[KEY]+cols_ap].copy()\n",
        "        for c in cols_ap:\n",
        "            ap[c] = pd.to_numeric(ap[c], errors='ignore')\n",
        "        feat = feat.merge(ap, on=KEY, how='left')\n",
        "\n",
        "def aggregate_first_6h(df, key_col, offset_candidates, prefix):\n",
        "    if df is None: return None\n",
        "    off = next((c for c in offset_candidates if c in df.columns), None)\n",
        "    if off is None: return None\n",
        "    sub = df[(pd.to_numeric(df[off], errors='coerce') >= 0) &\n",
        "             (pd.to_numeric(df[off], errors='coerce') <= 360)].copy()\n",
        "    if sub.empty: return None\n",
        "    num_cols = [c for c in sub.columns if c not in [key_col, off] and pd.api.types.is_numeric_dtype(sub[c])]\n",
        "    if not num_cols: return None\n",
        "    ag = sub.groupby(key_col)[num_cols].agg(['min','max','mean']).reset_index()\n",
        "    ag.columns = [key_col] + [f\"{prefix}__{c}__{stat}\" for c in num_cols for stat in ['min','max','mean']]\n",
        "    return ag\n",
        "\n",
        "vp_agg = aggregate_first_6h(vital_per, KEY, ['observationoffset','chartoffset','offset'], 'vitalp')\n",
        "if vp_agg is not None: feat = feat.merge(vp_agg, on=KEY, how='left')\n",
        "\n",
        "va_agg = aggregate_first_6h(vital_aper, KEY, ['observationoffset','chartoffset','offset'], 'vitala')\n",
        "if va_agg is not None: feat = feat.merge(va_agg, on=KEY, how='left')\n",
        "\n",
        "if lab is not None:\n",
        "    lb = lab.copy()\n",
        "    lb_off  = next((c for c in ['labresultoffset','chartoffset','observationoffset','offset'] if c in lb.columns), None)\n",
        "    val_col = next((c for c in ['labresult','labvalue','resultvalue','value'] if c in lb.columns), None)\n",
        "    lname   = 'labname' if 'labname' in lb.columns else None\n",
        "    if lb_off and val_col and lname:\n",
        "        lb[val_col] = pd.to_numeric(lb[val_col], errors='coerce')\n",
        "        common = lb[lb[val_col].notna()].groupby(lname)[KEY].nunique().sort_values(ascending=False).head(20).index.tolist()\n",
        "        aggs = []\n",
        "        for ln in common:\n",
        "            sub = lb[(lb[lname]==ln) & lb[val_col].notna()]\n",
        "            ag = aggregate_first_6h(sub, KEY, [lb_off], f\"lab__{str(ln).replace(' ','_').lower()}\")\n",
        "            if ag is not None: aggs.append(ag)\n",
        "        if aggs:\n",
        "            feat = reduce(lambda L,R: L.merge(R,on=KEY,how='left'), [feat]+aggs)\n",
        "\n",
        "if past_hist is not None:\n",
        "    ph = past_hist.copy()\n",
        "    text_col = next((c for c in ['pasthistorytext','pasthistoryitem','pasthistoryvalue','pasthistorypath'] if c in ph.columns), None)\n",
        "    if text_col:\n",
        "        ph[text_col] = ph[text_col].astype(str).str.lower()\n",
        "        agg_txt = ph.groupby(KEY)[text_col].apply(lambda s: ' | '.join(s.astype(str))).reset_index()\n",
        "        flags = {\n",
        "            'hx_chf': ['heart failure','congestive heart','chf'],\n",
        "            'hx_copd': ['copd','chronic obstructive'],\n",
        "            'hx_ckd': ['ckd','chronic kidney','renal failure'],\n",
        "            'hx_dm':  ['diabetes','dm '],\n",
        "            'hx_cancer': ['cancer','malignancy','carcinoma','tumor'],\n",
        "        }\n",
        "        for col, kws in flags.items():\n",
        "            agg_txt[col] = agg_txt[text_col].apply(lambda t: any(k in t for k in kws))\n",
        "        feat = feat.merge(agg_txt.drop(columns=[text_col]), on=KEY, how='left')\n",
        "\n",
        "\n",
        "code_col = 'icd9code' if 'icd9code' in diagnoses.columns else ('icd10code' if 'icd10code' in diagnoses.columns else None)\n",
        "diagnoses[code_col] = diagnoses[code_col].astype(str).str.replace('.','', regex=False).str.upper()\n",
        "\n",
        "CURATED_GROUPS = {\n",
        "    'sepsis': ['A41','038','R652','9959'],\n",
        "\n",
        "}\n",
        "\n",
        "def code_to_curated_group(code):\n",
        "    for grp, prefs in CURATED_GROUPS.items():\n",
        "        if any(code.startswith(p) for p in prefs):\n",
        "            return grp\n",
        "    return None\n",
        "\n",
        "diag_grp = diagnoses[[KEY, code_col]].copy()\n",
        "diag_grp['curated_group'] = diag_grp[code_col].apply(code_to_curated_group)\n",
        "\n",
        "labels = pd.DataFrame({KEY: patients[KEY].unique()})\n",
        "for grp in CURATED_GROUPS.keys():\n",
        "    stays = diag_grp.loc[diag_grp['curated_group']==grp, KEY].unique()\n",
        "    labels[grp] = labels[KEY].isin(stays).astype(int)\n",
        "\n",
        "df_all = labels.merge(feat, on=KEY, how=\"inner\")\n",
        "print(\"Final dataset shape:\", df_all.shape)\n",
        "df_all.to_csv(os.path.join(OUT,\"df_all_6h.csv\"), index=False)"
      ],
      "metadata": {
        "id": "23D0-gs1RLUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (roc_auc_score, average_precision_score, precision_recall_curve,\n",
        "                             f1_score, confusion_matrix, classification_report)\n",
        "\n",
        "OUT = os.path.join(BASE_DIR, \"outputs\")\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "df_all = labels.merge(feat, on=KEY, how='inner')\n",
        "\n",
        "label_cols = [c for c in labels.columns if c != KEY]\n",
        "cat_cols = [c for c in df_all.columns if df_all[c].dtype == 'object' and c not in [KEY] + label_cols]\n",
        "num_cols = [c for c in df_all.columns if c not in cat_cols + [KEY] + label_cols]\n",
        "\n",
        "print(\"Total samples:\", len(df_all))\n",
        "print(\"Num features:\", len(num_cols), \"| Cat features:\", len(cat_cols), \"| Labels:\", len(label_cols))\n",
        "\n",
        "pre = ColumnTransformer([\n",
        "    (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
        "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                      (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols)\n",
        "])\n",
        "\n",
        "tree_method = \"hist\"\n",
        "\n",
        "RESULTS = []\n",
        "TOP_SHAP_LABELS = 5\n",
        "pos_by_label = {lbl: int(df_all[lbl].sum()) for lbl in label_cols}\n",
        "top_for_shap = [lbl for lbl, _ in sorted(pos_by_label.items(), key=lambda x: x[1], reverse=True)[:TOP_SHAP_LABELS]]\n",
        "\n",
        "for lbl in label_cols:\n",
        "    print(\"\\n============================\")\n",
        "    print(f\"Training for label: {lbl} | positives: {pos_by_label[lbl]}\")\n",
        "    y = df_all[lbl].astype(int)\n",
        "    if y.sum() < 25:\n",
        "        print(\"Skipping label due to low positives (<25).\")\n",
        "        continue\n",
        "\n",
        "    X = df_all[num_cols + cat_cols]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y, random_state=42\n",
        "    )\n",
        "\n",
        "    neg = (y_train == 0).sum()\n",
        "    pos = (y_train == 1).sum()\n",
        "    spw = max(1.0, neg / max(1, pos))\n",
        "\n",
        "    clf = xgb.XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        tree_method=tree_method,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        scale_pos_weight=spw\n",
        "    )\n",
        "\n",
        "    pipe = Pipeline([(\"prep\", pre), (\"model\", clf)])\n",
        "\n",
        "    param_distributions = {\n",
        "        \"model__n_estimators\": [300, 500, 700],\n",
        "        \"model__max_depth\": [3, 4, 5, 6],\n",
        "        \"model__learning_rate\": [0.02, 0.05, 0.1],\n",
        "        \"model__subsample\": [0.6, 0.8, 1.0],\n",
        "        \"model__colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "        \"model__min_child_weight\": [1, 2, 5, 10],\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    search = RandomizedSearchCV(pipe, param_distributions, n_iter=12, cv=cv,\n",
        "                                scoring=\"roc_auc\", n_jobs=-1, verbose=1, random_state=42)\n",
        "    search.fit(X_train, y_train)\n",
        "    best_pipe = search.best_estimator_\n",
        "\n",
        "    y_proba = best_pipe.predict_proba(X_test)[:,1]\n",
        "    roc_auc = roc_auc_score(y_test, y_proba)\n",
        "    pr_auc  = average_precision_score(y_test, y_proba)\n",
        "\n",
        "    prec, rec, th = precision_recall_curve(y_test, y_proba)\n",
        "    f1 = (2*prec*rec)/(prec+rec+1e-12)\n",
        "    ix = int(np.nanargmax(f1))\n",
        "    best_th = th[ix] if ix < len(th) else 0.5\n",
        "\n",
        "    y_pred = (y_proba >= best_th).astype(int)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, digits=3, output_dict=True)\n",
        "\n",
        "    label_dir = os.path.join(OUT, f\"label_{lbl}\")\n",
        "    os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(label_dir, \"metrics.json\"), \"w\") as f:\n",
        "        json.dump({\n",
        "            \"label\": lbl, \"positives\": int(y.sum()),\n",
        "            \"roc_auc\": float(roc_auc), \"pr_auc\": float(pr_auc),\n",
        "            \"threshold\": float(best_th), \"confusion_matrix\": cm.tolist(),\n",
        "            \"classification_report\": report\n",
        "        }, f, indent=2)\n",
        "\n",
        "    if lbl.lower() == \"sepsis\":\n",
        "      import seaborn as sns\n",
        "\n",
        "      ax_labels = [\"0\", \"1\"]\n",
        "      plt.figure(figsize=(5,4))\n",
        "      sns.heatmap(cm, annot=True, fmt='d', cmap=\"RdYlGn_r\", cbar=False,\n",
        "                xticklabels=ax_labels, yticklabels=ax_labels)\n",
        "\n",
        "      plt.text(0.5, 0.5, 'TP', fontsize=14, ha='center', va='center', color='black')\n",
        "      plt.text(1.5, 0.5, 'FN', fontsize=14, ha='center', va='center', color='black')\n",
        "      plt.text(0.5, 1.5, 'FP', fontsize=14, ha='center', va='center', color='black')\n",
        "      plt.text(1.5, 1.5, 'TN', fontsize=14, ha='center', va='center', color='black')\n",
        "\n",
        "      plt.xlabel('Predicted')\n",
        "      plt.ylabel('Actual')\n",
        "      plt.title(f'Confusion Matrix for {lbl}')\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(os.path.join(label_dir, \"confusion_matrix.png\"), dpi=200, bbox_inches='tight')\n",
        "      plt.close()\n",
        "\n",
        "    import joblib\n",
        "    joblib.dump(best_pipe, os.path.join(label_dir, \"model.pkl\"))\n",
        "\n",
        "    if lbl in top_for_shap:\n",
        "        prep_step = best_pipe.named_steps['prep']\n",
        "        model_step = best_pipe.named_steps['model']\n",
        "        X_train_t = prep_step.transform(X_train)\n",
        "        num_names = num_cols\n",
        "        cat_oh = prep_step.transformers_[1][1].named_steps['oh'].get_feature_names_out(cat_cols) if cat_cols else []\n",
        "        feature_names = list(num_names) + list(cat_oh)\n",
        "\n",
        "        explainer = shap.TreeExplainer(model_step)\n",
        "        nsample = min(4000, X_train_t.shape[0])\n",
        "        sv = explainer.shap_values(X_train_t[:nsample])\n",
        "\n",
        "        plt.figure()\n",
        "        shap.summary_plot(sv, X_train_t[:nsample], feature_names=feature_names, show=False)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(label_dir, \"shap_summary.png\"), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    RESULTS.append({\n",
        "        \"label\": lbl,\n",
        "        \"positives\": int(y.sum()),\n",
        "        \"roc_auc\": float(roc_auc),\n",
        "        \"pr_auc\": float(pr_auc),\n",
        "        \"threshold\": float(best_th),\n",
        "        \"f1_at_threshold\": float(f1_score(y_test, y_pred)),\n",
        "        \"tn\": int(cm[0,0]), \"fp\": int(cm[0,1]), \"fn\": int(cm[1,0]), \"tp\": int(cm[1,1])\n",
        "    })\n",
        "\n",
        "res_df = pd.DataFrame(RESULTS).sort_values([\"positives\",\"pr_auc\"], ascending=[False, False])\n",
        "summary_path = os.path.join(OUT, \"multilabel_results_summary.csv\")\n",
        "res_df.to_csv(summary_path, index=False)\n",
        "print(f\"\\nSaved per-label results to {summary_path}\")\n",
        "display(res_df.head(10))"
      ],
      "metadata": {
        "id": "mahaN6rjRpXR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}